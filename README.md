
<p align="center">
<h3 align="center"> SparseDistilBERT(SDBERT)</h3>
<div align="center">
<p>Implemented Sparse attention with Knowledge distillation on BERT model on IMDB dataset</p>


</div>

------------------------------------------
Designed, implemented and evaluated a new deep learning based model entitled  "SparseDistilBERT" on IMDB dataset, which is faster than other full-attention model like BERT, and also able to give good perpofmance. 

This repository also contains implementation of **"Attention is all you need"** paper model with knowledge distillation on **"BBC-news"** dataset. 
</div>

------------------------------------------
### Results  :

![image](https://user-images.githubusercontent.com/73088379/142274548-0250db8e-ebec-439e-af3b-35f962061675.png)

